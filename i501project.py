# -*- coding: utf-8 -*-
"""i501Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0VJXouo1U2G0n2DPo1qqgGtgh74mpLZ
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load the Excel file
file_path = '/content/drive/My Drive/wfp_market_food_prices.csv'  # Update the path if needed
df = pd.read_csv(file_path, encoding='ISO-8859-1')

# Display the first few rows of the DataFrame
df.head()

"""Filtering dataset to countries that include Costa Rica, El Salvador,
Guateal ,
Honr, , s
nnd  and molCol.o.mbia
"""

# Define the list of countries to filter
countries = ['Costa Rica', 'El Salvador', 'Guatemala', 'Honduras', 'Panama', 'Colombia']

# Filter the DataFrame based on the adm0_name column
filtered_df = df[df['adm0_name'].isin(countries)]

# Display the filtered DataFrame
filtered_df.sample(10)

"""Writing filtered dataset into new CSV file for reference.

# Data Cleaning

Changing columns name of few columns for easier readibility
"""

# Rename the columns in filtered_df
filtered_df = filtered_df.rename(columns={
    'adm0_id' : 'Country_id',
    'adm0_name': 'Country',
    'adm1_id': 'locality_id',
    'adm1_name': 'locality_name',
    'pt_id': 'mk_type_id',
    'pt_name': 'mk_type_name'
})

# Display the updated filtered DataFrame with new column names
filtered_df.sample(10)

filtered_df.shape

"""Getting important info of the dataset and summary statistics of all columns."""

filtered_df.info()
filtered_df.describe(include='all')

"""We only see the column 'locality_name', which has null values. Other columns don't have null values. This shows that this dataset is very good to work with. Hence let's explore the missing values concept with column 'locality_name'. We also see that the data types for each column is exactly as we want. Hence we don't need to correct data types."""

filtered_df['locality_name'].isna().sum() #number of missing values

filtered_df['locality_name'].isna().value_counts()

(filtered_df.count() / filtered_df.shape[0]).sort_values()

# Remove rows with missing values
filtered_df_miss = filtered_df.dropna()

filtered_df_miss.describe(include='all')

"""**We can see that 3 out of the 6 countries we filtered initially aren't present after removing missing values.** This means we neglecting good content for our analysis. Therefore, we shall keep the rows of missing values.

Let us remove duplicates and **transform month and year into a column**
"""

# Remove duplicate rows if present
filtered_df_cleaned = filtered_df.drop_duplicates()

# Print the shape of the new dataset
print("Shape of the dataset after removing rows with missing values and duplicates:", filtered_df_cleaned.shape)

# Create a new 'month-year' period column from 'mp_year' and 'mp_month'
filtered_df_cleaned['month_year'] = filtered_df_cleaned['mp_year'].astype(str) + '-' + filtered_df_cleaned['mp_month'].astype(str)

# Convert 'month_year' to a Period type with monthly frequency
filtered_df_cleaned['month_year'] = pd.to_datetime(filtered_df_cleaned['month_year'], format='%Y-%m')

# Print the shape of the final cleaned dataset
print("Shape of the dataset after cleaning:", filtered_df_cleaned.shape)

filtered_df_cleaned.describe(include='all')

# Display the datatype of the 'month_year' column
print("Datatype of 'month_year' column:", filtered_df_cleaned['month_year'].dtype)

"""# Detecting Outliers for numerical columns that aren't ID's"""

import warnings
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Set the figure size
plt.figure(figsize=(15, 10))

# List of numerical columns to plot
numerical_columns = ['mp_month', 'mp_year', 'mp_price']

# Create box plots for each numerical column
for i, column in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)  # Create a subplot for each numerical column
    sns.boxplot(data=filtered_df_cleaned, x=column)
    plt.title(f'Box plot of {column}')

# Adjust layout to avoid overlap
plt.tight_layout()

# Show the plot
plt.show()

"""We notice 'mp_month', 'mp_year' don't project alarming outliers. However, 'mp_price' shows alarming outliers. Since the goal of our project is to predict food prices, Let us shift our focus into understanding this price column for the countries we have chosen from. The outliers can be due to differences in currency value in each country. So let us first convert each currency into USD, and then measure outliers. **We notice that only 2 of the 6 countries don't have USD, so let's convert them first.**

# Currency Converter
"""

# Define currency conversion function
def convert_currency(row):
    if row['cur_id'] == 67:  # COP
        return row['mp_price'] * 0.00024
    elif row['cur_id'] == 82:  # GTQ
        return row['mp_price'] * 0.13
    else:
        return row['mp_price']  # No conversion needed for other currencies

# Apply the conversion and update 'mp_price' column in place
filtered_df_cleaned['mp_price'] = filtered_df_cleaned.apply(convert_currency, axis=1)

# Display the updated dataframe to check the results
print(filtered_df_cleaned[['cur_id', 'cur_name', 'mp_price']].sample(10))

"""Let us analyze which country contributes to the outliers even after converting currencies."""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings

# Suppress FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Set the figure size
plt.figure(figsize=(15, 10))

# Create a box plot with countries as hue
sns.boxplot(data=filtered_df_cleaned[filtered_df_cleaned['Country'].isin(['Costa Rica', 'El Salvador', 'Guatemala', 'Honduras', 'Panama', 'Colombia'])],
                  x='Country', y='mp_price')

plt.title('Box plot of mp_price by Country')
plt.xticks(rotation=45)
plt.tight_layout()

# Show the plot
plt.show()

"""We see that Colombia contributes mainly to the outliers here. This can be due to the bad inflation. Honduras, Panama and Costa Rica showcase to have higher prices than El Salvador and Guatemala. We shall look into the country factor in the later stages to understand better. Now we shall test out group_by and aggregate functions.

Let us explore unique values and see if we can consolidate anywhere.
"""

# Printing unique values for each column
print("Unique values for locality_name:", filtered_df_cleaned['locality_name'].unique())
print("Unique values for mkt_name:", filtered_df_cleaned['mkt_name'].unique())
print("Unique values for cm_name:", filtered_df_cleaned['cm_name'].unique())
print("Unique values for um_name:", filtered_df_cleaned['um_name'].unique())
print("Unique values for mp_commoditysource:", filtered_df_cleaned['mp_commoditysource'].unique())

"""We notice that we can consolidate a number of commodity names ('cm_name') as a number of them are very similar."""

# Define a mapping for renaming and consolidating values
consolidation_mapping = {
    'Rice (white)': 'Rice',
    'Rice (paddy)': 'Rice',
    'Rice (milled 80-20)': 'Rice',
    'Rice (ordinary, second quality)': 'Rice',
    'Rice (ordinary, first quality)': 'Rice',
    'Maize (white)': 'Maize',
    'Maize (yellow)': 'Maize',
    'Maize flour (white)': 'Maize flour',
    'Onions (red)': 'Onions',
    'Onions (white)': 'Onions',
    'Sugar (brown)': 'Sugar',
    'Sugar (brown, loaf)': 'Sugar',
    'Beans (red)': 'Beans',
    'Beans (black)': 'Beans',
    'Beans (silk red)': 'Beans',
    'Beans (red, fresh)': 'Beans',
    'Meat (chicken)': 'Chicken',
    'Meat (beef)': 'Beef',
    'Meat (beef, minced)': 'Beef',
    'Meat (beef, chops with bones)': 'Beef',
    'Meat (pork)': 'Pork',
    'Milk (pasteurized)': 'Milk',
    'Milk (powder)': 'Milk',
    'Cheese (dry)': 'Cheese'
}

# Apply the mapping to the 'cm_name' column
filtered_df_cleaned['cm_name'] = filtered_df_cleaned['cm_name'].replace(consolidation_mapping)

# Check the updated unique values
print("Updated unique values for cm_name:", filtered_df['cm_name'].unique())

"""# Group_by & Aggregate functions"""

# Group by Country and calculate the mean, median, and standard deviation of mp_price
country_price_summary = filtered_df_cleaned.groupby('Country')['mp_price'].agg(['mean', 'median', 'std', 'count']).reset_index()
country_price_summary.columns = ['Country', 'Mean Price', 'Median Price', 'Std Dev Price', 'Count']
print(country_price_summary,"\n")

# Group by locality_name
locality_price_summary = filtered_df_cleaned.groupby('locality_name')['mp_price'].agg(['mean', 'median', 'std', 'count']).reset_index()
locality_price_summary.columns = ['Locality Name', 'Mean Price', 'Median Price', 'Std Dev Price', 'Count']
print(locality_price_summary,"\n")

# Group by mkt_name
mkt_price_summary = filtered_df_cleaned.groupby('mkt_name')['mp_price'].agg(['mean', 'median', 'std', 'count']).reset_index()
mkt_price_summary.columns = ['Market Name', 'Mean Price', 'Median Price', 'Std Dev Price', 'Count']
print(mkt_price_summary,"\n")

# Group by cm_name
cm_price_summary = filtered_df_cleaned.groupby('cm_name')['mp_price'].agg(['mean', 'median', 'std', 'count']).reset_index()
cm_price_summary.columns = ['Commodity Name', 'Mean Price', 'Median Price', 'Std Dev Price', 'Count']
print(cm_price_summary,"\n")

# Group by mk_type_name
mk_type_price_summary = filtered_df_cleaned.groupby('mk_type_name')['mp_price'].agg(['mean', 'median', 'std', 'count']).reset_index()
mk_type_price_summary.columns = ['Market Type Name', 'Mean Price', 'Median Price', 'Std Dev Price', 'Count']
print(mk_type_price_summary,"\n")

# Maximum Median Price by Country
max_country_index = country_price_summary['Median Price'].idxmax()
max_median_price_country = country_price_summary.loc[max_country_index]
print("Maximum Median Price by Country:", max_median_price_country['Country'], "with a price of:", max_median_price_country['Median Price'])

# Maximum Median Price by Locality Name
max_locality_index = locality_price_summary['Median Price'].idxmax()
max_median_price_locality = locality_price_summary.loc[max_locality_index]
print("Maximum Median Price by Locality Name:", max_median_price_locality['Locality Name'], "with a price of:", max_median_price_locality['Median Price'])

# Maximum Median Price by Market Name
max_market_index = mkt_price_summary['Median Price'].idxmax()
max_median_price_market = mkt_price_summary.loc[max_market_index]
print("Maximum Median Price by Market Name:", max_median_price_market['Market Name'], "with a price of:", max_median_price_market['Median Price'])

# Maximum Median Price by Commodity Name
max_commodity_index = cm_price_summary['Median Price'].idxmax()
max_median_price_commodity = cm_price_summary.loc[max_commodity_index]
print("Maximum Median Price by Commodity Name:", max_median_price_commodity['Commodity Name'], "with a price of:", max_median_price_commodity['Median Price'])

# Maximum Median Price by Market Type Name
max_market_type_index = mk_type_price_summary['Median Price'].idxmax()
max_median_price_market_type = mk_type_price_summary.loc[max_market_type_index]
print("Maximum Median Price by Market Type Name:", max_median_price_market_type['Market Type Name'], "with a price of:", max_median_price_market_type['Median Price'])

import pandas as pd
import matplotlib.pyplot as plt

# Create a function to plot median prices for a given filter and title
def plot_median_price(data, title):
    median_data = data.groupby('mp_year')['mp_price'].median().reset_index()

    plt.figure(figsize=(10, 6))
    plt.plot(median_data['mp_year'], median_data['mp_price'], marker='o', linestyle='-')
    plt.title(title)
    plt.xlabel('Year')
    plt.ylabel('Median Price')
    plt.xticks(rotation=45)
    plt.grid()
    plt.tight_layout()
    plt.show()

# Filter and plot for Colombia
colombia_data = filtered_df_cleaned[filtered_df_cleaned['Country'] == 'Costa Rica']
plot_median_price(colombia_data, 'Median Price Over Time - Costa Rica')

# Filter and plot for La Union
sucre_data = filtered_df_cleaned[filtered_df_cleaned['locality_name'] == 'La Union']
plot_median_price(sucre_data, 'Median Price Over Time - La Union')

# Filter and plot for La Union
sincelejo_data = filtered_df_cleaned[filtered_df_cleaned['mkt_name'] == 'La Union']
plot_median_price(sincelejo_data, 'Median Price Over Time - La Union')

# Filter and plot for Rice (milled 80-20)']
rice_data = filtered_df_cleaned[filtered_df_cleaned['cm_name'] == 'Maize flour']
plot_median_price(rice_data, 'Median Price Over Time - Rice (milled 80-20)')

# Filter and plot for Wholesale
wholesale_data = filtered_df_cleaned[filtered_df_cleaned['mk_type_name'] == 'Wholesale']
plot_median_price(wholesale_data, 'Median Price Over Time - Wholesale')

"""Every graph witnesses a rise from 2008 to 2012. But a dip after 2012. This explains there are some trends we can analyze to predict prices in the future. Hence let us shift our focus towards understanding the variables that can affect price fluctuations in the future.

# Correlation Analysis

Decoding categorical Variables
"""

from sklearn.preprocessing import LabelEncoder

# Copy the filtered dataframe
df_encoded = filtered_df_cleaned.copy()

# List of categorical columns to encode
categorical_columns = ['Country', 'locality_name', 'mkt_name', 'cm_name', 'cur_name', 'mk_type_name', 'um_name', 'mp_commoditysource', 'mp_month',]

# Label encode each categorical column
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    df_encoded[column] = le.fit_transform(df_encoded[column])
    label_encoders[column] = le  # Store the encoders in case you need to transform new data later

# Check the encoded data
print(df_encoded.head())

"""Checking for"""

# Step 1: Calculate the correlation matrix
correlation_matrix = df_encoded.corr()

# Step 2: Extract the correlations with mp_price
mp_price_correlation = correlation_matrix['mp_price'].sort_values(ascending=False)

# Display the correlations with mp_price
print("Correlation with mp_price:")
print(mp_price_correlation)

# Step 3: Visualize the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title('Correlation Matrix')
plt.show()

"""As we can see the correlation values are neither close to 1 nor -1. This indicates we can't provide a strong relationship between 'mp_price' and other variables. Hence let us try consolidating variables to understand relationships"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'filtered_df_cleaned' is your DataFrame

# Step 1: Combine the columns into new features
# Combine (Country, locality_name, mkt_name)
filtered_df_cleaned['location_combined'] = (
    filtered_df_cleaned['Country'].astype(str) + '_' +
    filtered_df_cleaned['locality_name'].astype(str) + '_' +
    filtered_df_cleaned['mkt_name'].astype(str)
)

# Combine (cm_name, cur_name, mp_commoditysource)
filtered_df_cleaned['commodity_combined'] = (
    filtered_df_cleaned['cm_name'].astype(str) + '_' +
    filtered_df_cleaned['cur_name'].astype(str) + '_' +
    filtered_df_cleaned['mp_commoditysource'].astype(str)
)

# Step 2: Label encode the combined columns
label_encoder = LabelEncoder()
filtered_df_cleaned['location_combined_encoded'] = label_encoder.fit_transform(filtered_df_cleaned['location_combined'])
filtered_df_cleaned['commodity_combined_encoded'] = label_encoder.fit_transform(filtered_df_cleaned['commodity_combined'])

# Step 3: Select relevant columns for correlation analysis
selected_columns = ['mp_price', 'location_combined_encoded', 'commodity_combined_encoded', 'mp_month', 'mp_year']

# Step 4: Compute the correlation matrix
correlation_matrix = filtered_df_cleaned[selected_columns].corr()

# Step 5: Get the correlation with mp_price
mp_price_correlation = correlation_matrix['mp_price'].sort_values(ascending=False)

# Display the correlations with mp_price
print(mp_price_correlation)

# Step 6: Visualize the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title('Correlation Matrix with Combined Features')
plt.show()

"""The consolidated columns make it worse. Therefore, we are going to stick with individual columns.

Let us now split the dataset into samples where one sample consists of records with year < 2012 and the other > 2012 to see if there are any changes in the correlations values.
"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Copy the filtered dataframe
df_encoded = filtered_df_cleaned.copy()

# Step 2: List of categorical columns to encode
categorical_columns = ['Country', 'locality_name', 'mkt_name', 'cm_name', 'cur_name', 'mk_type_name', 'um_name', 'mp_commoditysource', 'mp_month']

# Step 3: Label encode each categorical column
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    df_encoded[column] = le.fit_transform(df_encoded[column])
    label_encoders[column] = le  # Store the encoders if needed for future transformations

# Step 4: Split the data into two samples based on 'mp_year'
sample_1 = df_encoded[df_encoded['mp_year'] <= 2012]  # Sample 1: mp_year <= 2012
sample_2 = df_encoded[df_encoded['mp_year'] > 2012]   # Sample 2: mp_year > 2012

# Step 5: Select relevant columns for correlation analysis (including encoded categorical columns)
# Ensure 'mp_price' is included as it is the target of correlation analysis
selected_columns = ['mp_price', 'Country', 'locality_name', 'mkt_name', 'cm_name', 'cur_name',
                    'mk_type_name', 'um_name', 'mp_commoditysource', 'mp_month', 'mp_year']

# Step 6: Calculate correlation for both samples
correlation_sample_1 = sample_1[selected_columns].corr()
correlation_sample_2 = sample_2[selected_columns].corr()

# Step 7: Print correlation with 'mp_price' for both samples
print("Correlation with mp_price (mp_year <= 2012):")
print(correlation_sample_1['mp_price'].sort_values(ascending=False))

print("\nCorrelation with mp_price (mp_year > 2012):")
print(correlation_sample_2['mp_price'].sort_values(ascending=False))

# Step 8: Visualize the correlation matrices for both samples

# For sample 1 (mp_year <= 2012)
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_sample_1, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title('Correlation Matrix (mp_year <= 2012)')
plt.show()

# For sample 2 (mp_year > 2012)
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_sample_2, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title('Correlation Matrix (mp_year > 2012)')
plt.show()

"""Looks like this doesn't also increase the correlation values with 'mp_price'. So let's find the top most values that correlate with 'mp_price' in both cases.

We see that the columns with best correlation values is: cur_name, country, locality_name, um_name, mk_type_name. Let us use these variables in lo

# Linear Regression
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split

# List of individual features to test against 'mp_price'
features = ['cur_name', 'Country', 'locality_name', 'um_name', 'mk_type_name']

# Initialize the LinearRegression model
model = LinearRegression()

# Loop through each feature
for feature in features:
    X_single = df_encoded[[feature]]  # Select a single feature (reshape to 2D)
    y = df_encoded['mp_price']  # Target variable

    # Split the data for training and testing (70% training, 30% testing)
    X_train, X_test, y_train, y_test = train_test_split(X_single, y, test_size=0.3, random_state=42)

    # Fit the linear regression model for the current feature
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate the R-squared value
    r2 = r2_score(y_test, y_pred)

    # Plot the actual vs predicted values
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_test[feature], y=y_test, label='Actual', color='blue')
    sns.lineplot(x=X_test[feature], y=y_pred, label='Regression Line', color='red')

    # Add title and labels
    plt.title(f'Single-Variable Linear Regression for {feature}\nR-squared: {r2:.4f}')
    plt.xlabel(feature)
    plt.ylabel('mp_price')

    # Show plot
    plt.legend()
    plt.show()

"""We see that regression doesn't work here at all for us due to its very low R-squared values. The main reason also revolves around the fact that we are comparing categorical columns with prices. We need to find better methods to bring some trend between categorical and numerical.

# Shapiro (Anova or Kruskal)

To perform Shapiro test, checking normality, and then either ANOVA or Kruskal-Wallis to test for significance with mp_price.
"""

import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

def perform_tests(filtered_df_cleaned, categorical_var):
    # Perform Shapiro-Wilk test for normality
    shapiro_test = stats.shapiro(filtered_df_cleaned['mp_price'])
    print(f"Shapiro-Wilk Test for {categorical_var}: Statistic={shapiro_test.statistic}, p-value={shapiro_test.pvalue}")

    # Check if p-value is less than significance level (e.g., 0.05)
    if shapiro_test.pvalue < 0.05:
        print(f"Normality does not hold for {categorical_var}. Proceeding with Kruskal-Wallis test.")

        # Perform Kruskal-Wallis test
        kruskal_test = stats.kruskal(*(filtered_df_cleaned[filtered_df_cleaned[categorical_var] == cat]['mp_price']
                                        for cat in filtered_df_cleaned[categorical_var].unique()))
        print(f"Kruskal-Wallis Test for {categorical_var}: Statistic={kruskal_test.statistic}, p-value={kruskal_test.pvalue}")
    else:
        print(f"Normality holds for {categorical_var}. Proceeding with ANOVA.")

        # Perform ANOVA
        model = ols(f'mp_price ~ C({categorical_var})', data=filtered_df_cleaned).fit()
        anova_results = sm.stats.anova_lm(model, typ=2)
        print(anova_results)

categorical_vars = ['Country', 'locality_name', 'mkt_name', 'cm_name', 'cur_name', 'mk_type_id', 'um_name', 'mp_year', 'mp_month']

for var in categorical_vars:
    perform_tests(filtered_df_cleaned, var)

"""Significant Variables: The Kruskal-Wallis tests for Country, mkt_name, cm_name, cur_name, mk_type_id, um_name, and mp_year show significant differences in mp_price, indicating that these categorical variables impact pricing.

Locality Name Issue: The NaN result for the locality_name Kruskal-Wallis test suggests a need to check the data for this variable. Consider exploring the number of unique values and the distribution of prices within each locality.

Month Variable: The non-significant result for mp_month implies that month may not significantly affect mp_price in your dataset.

# Non-Linear methods

Let us use the significant variables and imp
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# Assuming filtered_df_cleaned is your DataFrame
# Selecting the relevant columns
significant_vars = ['Country', 'mkt_name', 'cm_name', 'cur_name', 'mk_type_id', 'um_name', 'mp_year']
X = filtered_df_cleaned[significant_vars]
y = filtered_df_cleaned['mp_price']

# Initialize LabelEncoders for each categorical column
label_encoders = {}
for col in X.columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le  # Store the encoder if you need to inverse transform later

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Polynomial Regression"""

# Polynomial Regression
degree = 4  # You can adjust the degree
polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())
polyreg.fit(X_train, y_train)

# Predictions
y_pred_poly = polyreg.predict(X_test)

# Evaluate
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

print(f"Polynomial Regression - Mean Squared Error: {mse_poly:.2f}, R^2 Score: {r2_poly:.2f}")

"""Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Decision Tree Regression
decision_tree = DecisionTreeRegressor(max_depth=5, min_samples_split=10)  # Parameters
decision_tree.fit(X_train, y_train)
y_pred_dt = decision_tree.predict(X_test)

# Evaluate
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print(f"Decision Tree - Mean Squared Error: {mse_dt:.2f}, R^2 Score: {r2_dt:.5f}")

"""Random Forest"""

from sklearn.ensemble import RandomForestRegressor

# Random Forest Regression
random_forest = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_split=10, random_state=42)  # Parameters
random_forest.fit(X_train, y_train)
y_pred_rf = random_forest.predict(X_test)

# Evaluate
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest - Mean Squared Error: {mse_rf:.2f}, R^2 Score: {r2_rf:.5f}")

"""SVM"""

from sklearn.svm import SVR

# Support Vector Machine Regression
svm_model = SVR(kernel='rbf', C=100, gamma='scale')  # Parameters
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# Evaluate
mse_svm = mean_squared_error(y_test, y_pred_svm)
r2_svm = r2_score(y_test, y_pred_svm)

print(f"Support Vector Machine - Mean Squared Error: {mse_svm:.2f}, R^2 Score: {r2_svm:.5f}")

"""Gradient Boosting"""

from sklearn.ensemble import GradientBoostingRegressor

# Gradient Boosting Regression
gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)  # Parameters
gradient_boosting.fit(X_train, y_train)
y_pred_gb = gradient_boosting.predict(X_test)

# Evaluate
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

print(f"Gradient Boosting - Mean Squared Error: {mse_gb:.2f}, R^2 Score: {r2_gb:.5f}")

"""All non-linear methods except SVM showcase really good R-square values but huge MSE values. This is because the data has has large range of values, even small differences between the predicted and actual prices can result in a high MSE. Since the models captures the trend well, R² can still be high even if MSE is elevated.

##Plotting for all models
"""

# Plotting
plt.figure(figsize=(20, 12))

# Polynomial Regression
plt.subplot(2, 2, 1)
plt.scatter(y_test, y_pred_poly, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)  # 45-degree line
plt.title('Polynomial Regression: Actual vs Predicted Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')

# Decision Tree Regression
plt.subplot(2, 2, 2)
plt.scatter(y_test, y_pred_dt, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.title('Decision Tree Regression: Actual vs Predicted Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')

# Random Forest Regression
plt.subplot(2, 2, 3)
plt.scatter(y_test, y_pred_rf, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.title('Random Forest Regression: Actual vs Predicted Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')

# Support Vector Machine Regression
plt.subplot(2, 2, 4)
plt.scatter(y_test, y_pred_svm, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.title('Support Vector Machine Regression: Actual vs Predicted Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')

plt.tight_layout()
plt.show()

"""#Printing predicted prices for each model except SVM

Polynomial Regression
"""

# Assuming filtered_df_cleaned is your DataFrame and you have already defined your X and y
significant_vars = ['Country', 'mkt_name', 'cm_name', 'cur_name', 'mk_type_id', 'um_name', 'mp_year']
X = filtered_df_cleaned[significant_vars]
y = filtered_df_cleaned['mp_price']

# Initialize LabelEncoders for each categorical column
label_encoders = {}
for col in X.columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le  # Store the encoder

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regression
random_forest = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_split=10, random_state=42)
random_forest.fit(X_train, y_train)

# Prepare data for prediction (mp_year = 2024)
# Create a new DataFrame with unique combinations of 'Country', 'cm_name', and other relevant columns for 2024
unique_countries = filtered_df_cleaned['Country'].unique()
unique_cm_names = filtered_df_cleaned['cm_name'].unique()
unique_cur_names = filtered_df_cleaned['cur_name'].unique()
unique_mkt_names = filtered_df_cleaned['mkt_name'].unique()
unique_um_names = filtered_df_cleaned['um_name'].unique()
unique_mk_types = filtered_df_cleaned['mk_type_id'].unique()

# Create a DataFrame for prediction
predict_data = pd.DataFrame([(country, mkt_name, cm_name, cur_name, mk_type, um_name, 2024)
                             for country in unique_countries
                             for mkt_name in unique_mkt_names
                             for cm_name in unique_cm_names
                             for cur_name in unique_cur_names
                             for mk_type in unique_mk_types
                             for um_name in unique_um_names],
                            columns=['Country', 'mkt_name', 'cm_name', 'cur_name', 'mk_type_id', 'um_name', 'mp_year'])

# Encode the categorical variables
for col in predict_data.columns:
    if col in label_encoders:
        # Handle 'mp_year' separately to include 2024
        if col == 'mp_year':
            le_year = LabelEncoder()
            le_year.fit(list(label_encoders[col].classes_) + [2024])  # Include 2024
            predict_data[col] = le_year.transform(predict_data[col])
        else:
            predict_data[col] = label_encoders[col].transform(predict_data[col])

# Ensure the same order of columns as in training
predict_data = predict_data[significant_vars]

# Make predictions
predicted_prices = random_forest.predict(predict_data)

# Adding the predictions to the DataFrame
predict_data['mp_price'] = predicted_prices

# Decode categorical variables back to original values
for col in predict_data.columns:
    if col in label_encoders:
        # Use the original encoder for each column
        if col == 'mp_year':
            predict_data[col] = le_year.inverse_transform(predict_data[col])  # Use the new label encoder for 'mp_year'
        else:
            predict_data[col] = label_encoders[col].inverse_transform(predict_data[col])

# Group by Country and cm_name to get average predictions if needed
grouped_predictions = predict_data.groupby(['Country', 'cm_name']).agg({'mp_price': 'mean'}).reset_index()

# Display the results
print(grouped_predictions)